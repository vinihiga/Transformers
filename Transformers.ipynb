{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fb26d5-aa1c-40aa-ac1b-623c34243d76",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "### Ideia do artigo \"Attention is all you need\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b198eb6e-6df5-4da6-9e62-468914eaab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052341c-b02b-48e4-940e-2d62062ecab1",
   "metadata": {},
   "source": [
    "### Implementando o Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebaf3ca8-9b8b-41cb-9226-a05533d981d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiado do artigo: https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
    "# Créditos: Frank Odom\n",
    "\n",
    "def position_encoding(\n",
    "    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** (dim / dim_model))\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2f129-0493-436f-b243-cfd53da251e0",
   "metadata": {},
   "source": [
    "### Implementando o mecanismo de Attention\n",
    "\n",
    "![Attention Mechanism](attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0811dc6-cf1c-4ccf-93fc-b3bfa68b16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, q_dim: int, k_dim: int, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.q_linear_layer = nn.Linear(input_dim, q_dim)\n",
    "        self.k_linear_layer = nn.Linear(input_dim, k_dim)\n",
    "        self.v_linear_layer = nn.Linear(input_dim, k_dim)\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        return self.__scaled_dot_product_attention(\n",
    "            self.q_linear_layer(q),\n",
    "            self.k_linear_layer(k),\n",
    "            self.v_linear_layer(v)\n",
    "        )\n",
    "    \n",
    "    def __scaled_dot_product_attention(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        # Obtendo produto escalar\n",
    "        product_q_k = torch.bmm(q, k.transpose(1, 2))\n",
    "    \n",
    "        # Scaling do produto\n",
    "        scaled_params = product_q_k / (k.size(-1) ** 0.5)\n",
    "    \n",
    "        # Função softmax com os parâmetros obtidos e obtendo o produto escalar com V\n",
    "        logits = nn.functional.softmax(scaled_params)\n",
    "        result = torch.bmm(logits, v)\n",
    "    \n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b6ad1c-de2d-4e32-a29f-2c4f5265c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads: int, q_dim: int, k_dim: int, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "        # Criando uma lista de módulo com 'N' AttentionHeads\n",
    "        self.attention_heads = nn.ModuleList(\n",
    "            np.full(num_heads, AttentionHead(q_dim, k_dim, input_dim))\n",
    "        )\n",
    "    \n",
    "        self.linear_layer = nn.Linear(num_heads * k_dim, input_dim)\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        return self.linear_layer(\n",
    "            torch.cat([attention_head(q, k, v) for attention_head in self.attention_heads], dim = -1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e0132-61a3-43e7-9c42-c486d26f05ab",
   "metadata": {},
   "source": [
    "### Implementando o módulo residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87b2c2ef-f0bf-4713-9a8a-ec448971e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: Tensor) -> Tensor:\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3b888b-7259-4b1a-b836-3dfbe85b5d17",
   "metadata": {},
   "source": [
    "### Implementando o encoder e decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f4696f-4c57-4364-9030-49d6f9498844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_dim: int = 512,\n",
    "        ff_dim: int = 2048, \n",
    "        num_heads: int = 4,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        q_dim = max(model_dim // num_heads, 1)\n",
    "        k_dim = max(model_dim // num_heads, 1)\n",
    "\n",
    "        self.attention_layer = Residual(\n",
    "            MultiAttentionHead(num_heads, q_dim, k_dim, input_dim = model_dim),\n",
    "            model_dim,\n",
    "            dropout\n",
    "        )\n",
    "\n",
    "        self.ff_layer = Residual(\n",
    "            self.__instantiate_feed_forward(model_dim, ff_dim),\n",
    "            model_dim,\n",
    "            dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        attention_output = self.attention_layer(x, x, x)\n",
    "        ff_output = self.ff_layer(attention_output)\n",
    "        return ff_output\n",
    "    \n",
    "    def __instantiate_feed_forward(self, dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(dim_input, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, dim_input),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd3ecd6-caec-4b88-9632-9a67c1555c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_dim: int = 512,\n",
    "        ff_dim: int = 2048, \n",
    "        num_heads: int = 4,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        q_dim = max(model_dim // num_heads, 1)\n",
    "        k_dim = max(model_dim // num_heads, 1)\n",
    "\n",
    "        self.attention_layer_1 = Residual(\n",
    "            MultiAttentionHead(num_heads, q_dim, k_dim, input_dim = model_dim),\n",
    "            model_dim,\n",
    "            dropout\n",
    "        )\n",
    "\n",
    "        self.attention_layer_2 = Residual(\n",
    "            MultiAttentionHead(num_heads, q_dim, k_dim, input_dim = model_dim),\n",
    "            model_dim,\n",
    "            dropout\n",
    "        )\n",
    "\n",
    "        self.ff_layer = Residual(\n",
    "            self.__instantiate_feed_forward(model_dim, ff_dim),\n",
    "            model_dim,\n",
    "            dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, memory: Tensor) -> Tensor:\n",
    "        layer_1_output = self.attention_layer_1(x, x, x)\n",
    "        layer_2_output = self.attention_layer_2(layer_1_output, memory, memory)\n",
    "        ff_layer_output = self.ff_layer(layer_2_output)\n",
    "        return ff_layer_output\n",
    "\n",
    "    def __instantiate_feed_forward(self, dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(dim_input, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, dim_input),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad03b7-99c2-40cc-a9cf-7bdcaa0c5d0a",
   "metadata": {},
   "source": [
    "### Implementando o Transformer\n",
    "\n",
    "![Transformer Architecture](transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de436f02-55b0-4d4f-8b69-072497454c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers = 4,\n",
    "        num_decoder_layers = 4,\n",
    "        model_dim: int = 512,\n",
    "        ff_dim: int = 2048, \n",
    "        num_heads: int = 4,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoders = nn.ModuleList(\n",
    "            np.full(\n",
    "                num_encoder_layers,\n",
    "                Encoder(model_dim, ff_dim, num_heads, dropout)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.decoders = nn.ModuleList( \n",
    "            np.full(\n",
    "                num_decoder_layers,\n",
    "                Decoder(model_dim, ff_dim, num_heads, dropout)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.external_ff = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, x_1: Tensor, x_2: Tensor) -> Tensor:\n",
    "\n",
    "        seq_len_x1, dim_len_x1 = x_1.size(1),  x_1.size(2)\n",
    "        x_1 += position_encoding(seq_len_x1, dim_len_x1)\n",
    "\n",
    "        seq_len_x2, dim_len_x2 = x_2.size(1),  x_2.size(2)\n",
    "        x_2 += position_encoding(seq_len_x2, dim_len_x2)\n",
    "\n",
    "        encoder_output = x_1\n",
    "\n",
    "        for encoder in self.encoders:\n",
    "            encoder_output = encoder(encoder_output)\n",
    "\n",
    "        decoder_output = x_2\n",
    "\n",
    "        for decoder in self.decoders:\n",
    "            decoder_output = decoder(decoder_output, encoder_output)\n",
    "\n",
    "        y = torch.softmax(self.external_ff(decoder_output), dim = -1)\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cda16-02e2-408c-ab99-ed3d806f7a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3db394ea-61a5-4f78-bfd1-281051238e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinny\\AppData\\Local\\Temp\\ipykernel_1640\\3474543643.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = nn.functional.softmax(scaled_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = torch.rand(16, 16, 512)\n",
    "x_2 = torch.rand(16, 16, 512)\n",
    "y = Transformer()(x_1, x_2)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b9da2-2af3-462a-93f6-9e280cc5cbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
